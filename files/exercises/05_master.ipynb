{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner logo](https://raw.githubusercontent.com/CitrineInformatics/community-tools/master/templates/fig/citrine_banner_2.png \"Banner logo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Learning Challenge\n",
    "\n",
    "*Authors: Zach del Rosario (zdelrosario@citrine.io)*\n",
    "\n",
    "Now that we've \"eaten our vegetables\", we're ready to start using machine learning to study materials science problems. We'll use the [Agrawal et al. (2014) IMMI](https://citrination.com/datasets/150670/show_search?searchMatchOption=fuzzyMatch) dataset to study the relationship between alloy composition and fatigue strength.\n",
    "\n",
    "### Learning outcomes\n",
    "By working through this notebook, you will be able to:\n",
    "\n",
    "* Understand *featurization*\n",
    "* Featurize inorganic materials data\n",
    "* Understand *sequential learning*\n",
    "* Design a machine learning model to support sequential learning in the support of designing novel materials\n",
    "\n",
    "Tips:\n",
    "\n",
    "* Designing a model is an *iterative* process; if you are not happy with your results, consider alternative choices, implement them, and compare them against a baseline to tell if your new approach is an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Model training tools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "\n",
    "# For jupyter-matplotlib compatibility\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom tools\n",
    "from workshop_utils import formulas2df, sequentialLearningSimulator, plotHistory\n",
    "\n",
    "# Agrawal data from previous exercise\n",
    "filename_data = \"./agrawal_data.csv\"\n",
    "\n",
    "# Helper functions\n",
    "def nde(y_true, y_pred):\n",
    "    \"\"\"Non-dimensional Model Error\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    response_std = np.std(y_true)\n",
    "    \n",
    "    return np.sqrt(mse) / response_std\n",
    "\n",
    "nde_score = make_scorer(nde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: ML on Materials Data\n",
    "Now that we have learned some (but not _all_!) concepts about training machine learning models, we are finally ready to apply ML to a materials problem.\n",
    "\n",
    "### Q1: Load the Fatigue Strength data\n",
    "Load the Agrawal fatigue strength data from the earlier exercise. Its filename is stored in the variable `filename_data`. Make sure to load the data to a Pandas DataFrame, and name it `df_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK: Load the Agrawal data from the file at filename_data\n",
    "# solution-begin\n",
    "df_data = pd.read_csv(filename_data)\n",
    "# solution-end\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple featurization\n",
    "Alloy composition is encoded in a string in the column `chemical_formula`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[['chemical_formula']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a linear regression _directly_ to this _string representation_ is not feasible -- these are not continuous values! Instead, we will _featurize_ the chemical formulas by representing each element fraction as a separate column.\n",
    "\n",
    "Note that doing this _programmatically_ is a bit tricky (it requires [regular expressions](https://en.wikipedia.org/wiki/Regular_expression)) -- I've written a simple parser to do this in a single function call. Feel free to inspect the code in `workshop_utils.py` if you'd like to see how this works! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_composition = formulas2df(df_data['chemical_formula'])\n",
    "X_compositions = df_composition.values  # Inputs for model\n",
    "Y_fatigue = df_data['Fatigue Strength'] # Response (to predict)\n",
    "\n",
    "df_composition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_compositions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ten physical features on which to fit our model.\n",
    "\n",
    "### Q2: Fit a model on alloy composition\n",
    "In the previous exercise (Exercises in Machine Learning), we learned how to fit a polynomial of user-selected order by *further featurizing* our data. Using the concepts we learned in that previous exercise, choose a *principled* choice of polynomial order for fitting the `X_compositions` data. Provide the further featurized data in the variable `X_data`. \n",
    "\n",
    "Once you have completed this task, move on the *sequential learning simulation* below, which will use your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK: Provide featurized data in the variable X_data. Choose how to featurize the data.\n",
    "# solution-begin\n",
    "# I choose to cross-validate the model choices, in order to select an optimal order for the model.\n",
    "Ord_all = [0,1,2,3]\n",
    "n_cv = 5\n",
    "NDE_cv_test_all = np.zeros((len(Ord_all), n_cv))\n",
    "NDE_cv_train_all = np.zeros((len(Ord_all), n_cv))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for i in range(len(Ord_all)):\n",
    "    # Fit model\n",
    "    poly   = PolynomialFeatures(Ord_all[i])\n",
    "    X_poly = poly.fit_transform(X_compositions)\n",
    "    reg    = LinearRegression().fit(X_poly, Y_fatigue)\n",
    "    \n",
    "    # Cross-validate\n",
    "    scores = cross_validate(\n",
    "        reg, poly.fit_transform(X_compositions), Y_fatigue,\n",
    "        cv = n_cv,\n",
    "        scoring = nde_score,\n",
    "        return_train_score = True\n",
    "    )\n",
    "    NDE_cv_test_all[i] = scores['test_score']\n",
    "    NDE_cv_train_all[i] = scores['train_score']\n",
    "    # Plot all CV test instances\n",
    "    plt.plot([Ord_all[i]] * n_cv, NDE_cv_test_all[i], 'k.')\n",
    "NDE_cv_test = np.mean(NDE_cv_test_all, axis = 1)\n",
    "NDE_cv_train = np.mean(NDE_cv_train_all, axis = 1)\n",
    "\n",
    "plt.plot(Ord_all, NDE_cv_test, label = 'Test')\n",
    "plt.legend(loc = 0)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Poly Order')\n",
    "plt.ylabel('ND Error')\n",
    "plt.show()\n",
    "\n",
    "ind_min = np.argmin(NDE_cv_test)\n",
    "order = Ord_all[ind_min]\n",
    "print(\"ord_min = {}\".format(order))\n",
    "print(\"NDE_min = {}\".format(NDE_cv_test[ind_min]))\n",
    "\n",
    "# Based on these results, I choose order = 2, and provide the featurized data\n",
    "poly   = PolynomialFeatures(order)\n",
    "X_data = poly.fit_transform(X_compositions)\n",
    "# solution-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete __Q2__ by further featurizing the data, you can provide this information to the following *sequential learning simulator*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Learning Simulation\n",
    "*Sequential learning* (SL) is an iterative process where one uses a machine learning model to help select future experiments. You can think of this as using *AI as a lab partner*, where a trained machine learning model can be used to detect patterns in available experimental data, and suggest promising future candidates. The figure below depicts this process schematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/CitrineInformatics/ga-tech-workshop/master/fig/sequential-learning.png\" alt=\"sequential learning schematic\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential learning has been shown to be 3x to 10x faster at discovering an optimal material, as compared with random guessing [Ling et al. (2017) *IMMI*]. Thus, SL is an important use-case for materials informatics.\n",
    "\n",
    "The specific steps for sequential learning are:\n",
    "1. Collect relevant data for the problem at hand, build a database\n",
    "2. Fit a machine learning model to available data\n",
    "3. Use the trained model to evaluate un-tested candiates; rank candidates \n",
    "4. Test some number of these candidates experimentally\n",
    "5. Add the new data to the database\n",
    "6. Repeat from step 2 until satisfied\n",
    "\n",
    "Sequential learning introduces Step 3 in place of random guessing, where a machine learning model can be used *in collaboration* with physical intuition to rank candidates.\n",
    "\n",
    "Below, we provide code to *simulate* sequential learning: Given the steel fatigue data, we select a random subset of the data for initial training, fit a model to rank the remaining candidates, add the best candidate to our database, and repeat the process for a number of iterations. The code below will use your `X_data` to fit a model for SL, and will repeat the same procedure for two other models for comparison.\n",
    "\n",
    "*Note that the following code will take a few seconds to execute.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "X_random = np.atleast_2d(np.random.random(len(Y_fatigue))).T\n",
    "historyR = sequentialLearningSimulator(X_random, Y_fatigue)       # Random guessing\n",
    "history1 = sequentialLearningSimulator(X_data, Y_fatigue)         # Your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following convenience functions will plot the best candidate's fatigue strength at different iterations of sequential learning. Better models will discover the top candidate faster, reaching the dotted ceiling quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plotHistory(historyR, Y_fatigue, \"Random\", color = \"grey\")\n",
    "plotHistory(history1, Y_fatigue, \"User\",   color = \"blue\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Fatigue Strength\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does your model out-perform random guessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced featurization\n",
    "The featurization above is fairly simple; we can actually bring in *much more* information about materials by leveraging our physical insight. The [Matminer](https://hackingmaterials.lbl.gov/matminer/) package is a set of tools for data-mining on chemicals data. Their library provides tools to produce _descriptors_ (features) based on chemical compositions. The following (Optional!) code demonstrates how to use Matminer to generate numerous features based on inorganic chemical labels (compositions). The following code demonstrates how to featurize chemical data using Matminer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "from matminer.featurizers.base import MultipleFeaturizer\n",
    "from matminer.featurizers import composition as cf\n",
    "from pymatgen import Composition\n",
    "\n",
    "def get_composition(c):\n",
    "    \"\"\"Attempt to parse a composition, return None if failed.\"\"\"\n",
    "    try:\n",
    "        return Composition(c)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "## Wrangle the composition strings\n",
    "df_data['composition'] = df_data['chemical_formula'].apply(get_composition)\n",
    "\n",
    "## Select features to compute\n",
    "featurizer = MultipleFeaturizer([\n",
    "    cf.Stoichiometry(),\n",
    "    cf.ElementProperty.from_preset(\"magpie\"),\n",
    "    cf.ValenceOrbital(props = ['avg']),\n",
    "    cf.IonProperty(fast = True)\n",
    "])\n",
    "\n",
    "## Run the featurizer\n",
    "X_matminer_features = np.array(featurizer.featurize_many(df_data['composition']))\n",
    "print(\"Featurization complete\")\n",
    "df_matminer_features = pd.DataFrame(\n",
    "    data = X_matminer_features,\n",
    "    columns = featurizer.feature_labels()\n",
    ")\n",
    "df_matminer_features.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matminer has provided `145` physics-based features for us to use for modeling. How does this perform in sequential learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = sequentialLearningSimulator(X_matminer_features, Y_fatigue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plotHistory(history1, Y_fatigue, \"User\",   color = \"blue\")\n",
    "plotHistory(history2, Y_fatigue, \"Matminer\", color = \"green\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Fatigue Strength\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find that a pure linear fit to the Matminer featurization does about as well as my own model. This suggests that simply adding additional features does not necessarily lead to improvement, at least in the context of SL performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Improving Sequential Learning Performance\n",
    "The remainder of this exercise is a *challenge* -- use the skills we learned throughout the workshop to improve your model.\n",
    "\n",
    "### Q3: Improve your model\n",
    "Modify your machine learning model to improve SL performance. Compare your results against the random baseline, your previous model, and your colleagues' results.\n",
    "\n",
    "Feel free to use any of the following ideas for improving your model, or invent your own ideas!\n",
    "\n",
    "### Suggested ideas:\n",
    "\n",
    "__Processing Characteristics__: Above I only used the *composition* of the metals, and ignored any *processing information*. You could re-introduce the processing characteristics to see how this affects SL performance.\n",
    "\n",
    "__Different Model__: The SL simulator uses linear regression as its default model form; you can change this by modifying the keyword argument `model`. For instance, `linearRegression` from scikit is used via\n",
    "\n",
    "```\n",
    "hist = sequentialLearningSimulator(X, Y, model = linearRegression())\n",
    "```\n",
    "\n",
    "One possible change you could make is to select a different model from the [scikit](https://scikit-learn.org/stable/supervised_learning.html) list of models. Read up on a few other options, and choose ones that seem promising.\n",
    "\n",
    "__Response Transformation__: Often, *transforming the response* `Y` can aid in modeling. The basic idea here is that picking the \"correct\" transform can simplify the relationship between `X` and `Y`, helping the machine learning model by reducing the complexity of what it needs to fit. Some common choices include powers `Y^p`, logarithms `log(Y)`, and other [more elaborate](https://en.wikipedia.org/wiki/Power_transform#Box–Cox_transformation) choices. There are some statistical reasons to try transforming the response, but know that this perspective is [subtle and tricky](https://boostedml.com/2019/04/linear-regression-log-transforming-response.html).\n",
    "\n",
    "*Note:* When transforming the response, make sure to provide the *original* `Y_fatigue` data to `plotHistory()`, otherwise results across different transforms will not be comparable. You should create a modified variable `Y_transformed = transform(Y_fatigue)`, and provide this to `sequentialLearningSimulator()`, but **not** to `plotHistory()`.\n",
    "\n",
    "__Feature Selection__: The Matminer example above demonstrates that simply adding more features to a model is not *guaranteed* to improve SL performance. In practice, it may be necessary to do some [feature selection](https://en.wikipedia.org/wiki/Feature_selection) -- chose a subset of available features to provide to the model. This is can be a very involved process, but feel free to explore a couple options.\n",
    "\n",
    "A *simple* way to do feature selection might be to visually inspect the relationship between single inputs `X[:, ind]` and the response `Y_fatigue` (using matplotlib) -- features that exhibit a clear pattern with the response are likely to be informative. This is a *labor intensive* way to select features, but one which does not require advanced mathematics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
