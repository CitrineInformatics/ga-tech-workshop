{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner logo](https://raw.githubusercontent.com/CitrineInformatics/community-tools/master/templates/fig/citrine_banner_2.png \"Banner logo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Learning Challenge\n",
    "\n",
    "*Authors: Zach del Rosario (zdelrosario@citrine.io)*\n",
    "\n",
    "Now that we've \"eaten our vegetables\", we're ready to start using machine learning to study materials science problems. We'll use the [Agrawal et al. (2014) IMMI](https://citrination.com/datasets/150670/show_search?searchMatchOption=fuzzyMatch) dataset to study the relationship between alloy composition and fatigue strength.\n",
    "\n",
    "### Learning outcomes\n",
    "By working through this notebook, you will be able to:\n",
    "\n",
    "* Understand *featurization*\n",
    "* Featurize inorganic materials data\n",
    "* Understand *sequential learning*\n",
    "* Design a machine learning model to support sequential learning in the support of designing novel materials\n",
    "\n",
    "Tips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Model training tools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "\n",
    "# For jupyter-matplotlib compatibility\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom tools\n",
    "from workshop_utils import formulas2df, sequentialLearningSimulator, plotHistory\n",
    "\n",
    "# Agrawal data from previous exercise\n",
    "filename_data = \"./agrawal_data.csv\"\n",
    "\n",
    "# Helper functions\n",
    "def nde(y_true, y_pred):\n",
    "    \"\"\"Non-dimensional Model Error\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    response_std = np.std(y_true)\n",
    "    \n",
    "    return np.sqrt(mse) / response_std\n",
    "\n",
    "nde_score = make_scorer(nde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: ML on Materials Data\n",
    "Now that we have learned some (but not _all_!) concepts about training machine learning models, we are finally ready to apply ML to a materials problem.\n",
    "\n",
    "### Q1: Load the Fatigue Strength data\n",
    "Load the Agrawal fatigue strength data from the earlier exercise. Its filename is stored in the variable `filename_data`. Make sure to load the data to a Pandas DataFrame, and name it `df_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK: Load the Agrawal data from the file at filename_data\n",
    "# solution-begin\n",
    "df_data = pd.read_csv(filename_data)\n",
    "# solution-end\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple featurization\n",
    "Alloy composition is encoded in a string in the column `chemical_formula`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[['chemical_formula']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a linear regression _directly_ to this _string representation_ is not feasible -- these are not continuous values! Instead, we will _featurize_ the chemical formulas by representing each element fraction as a separate column.\n",
    "\n",
    "Note that doing this _programmatically_ is a bit tricky (it requires [regular expressions](https://en.wikipedia.org/wiki/Regular_expression)) -- I've written a simple parser to do this in a single function call. Feel free to inspect the code in `workshop_utils.py` if you'd like to see how this works! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_composition = formulas2df(df_data['chemical_formula'])\n",
    "X_compositions = df_composition.values  # Inputs for model\n",
    "Y_fatigue = df_data['Fatigue Strength'] # Response (to predict)\n",
    "\n",
    "df_composition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_compositions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ten physical features on which to fit our model.\n",
    "\n",
    "### Q2: Fit a model on alloy composition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TASK: Provide featurized data in the variable X_data. Choose how to featurize the data.\n",
    "# solution-begin\n",
    "# I choose to cross-validate the model choices, in order to select an optimal order for the model.\n",
    "Ord_all = [0,1,2,3]\n",
    "n_cv = 5\n",
    "NDE_cv_test_all = np.zeros((len(Ord_all), n_cv))\n",
    "NDE_cv_train_all = np.zeros((len(Ord_all), n_cv))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for i in range(len(Ord_all)):\n",
    "    # Fit model\n",
    "    poly   = PolynomialFeatures(Ord_all[i])\n",
    "    X_poly = poly.fit_transform(X_compositions)\n",
    "    reg    = LinearRegression().fit(X_poly, Y_fatigue)\n",
    "    \n",
    "    # Cross-validate\n",
    "    scores = cross_validate(\n",
    "        reg, poly.fit_transform(X_compositions), Y_fatigue,\n",
    "        cv = n_cv,\n",
    "        scoring = nde_score,\n",
    "        return_train_score = True\n",
    "    )\n",
    "    NDE_cv_test_all[i] = scores['test_score']\n",
    "    NDE_cv_train_all[i] = scores['train_score']\n",
    "    # Plot all CV test instances\n",
    "    plt.plot([Ord_all[i]] * n_cv, NDE_cv_test_all[i], 'k.')\n",
    "NDE_cv_test = np.mean(NDE_cv_test_all, axis = 1)\n",
    "NDE_cv_train = np.mean(NDE_cv_train_all, axis = 1)\n",
    "\n",
    "plt.plot(Ord_all, NDE_cv_test, label = 'Test')\n",
    "plt.legend(loc = 0)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Poly Order')\n",
    "plt.ylabel('ND Error')\n",
    "plt.show()\n",
    "\n",
    "ind_min = np.argmin(NDE_cv_test)\n",
    "print(\"ord_min = {}\".format(Ord_all[ind_min]))\n",
    "print(\"NDE_min = {}\".format(NDE_cv_test[ind_min]))\n",
    "\n",
    "# Based on these results, I choose order = 2, and provide the featurized data\n",
    "poly   = PolynomialFeatures(2)\n",
    "X_data = poly.fit_transform(X_compositions)\n",
    "# solution-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete __Q2__ by further featurizing the data, you can provide this information to the following *sequential learning simulator*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Learning Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_history = sequentialLearningSimulator(X_data, Y_fatigue)\n",
    "plotHistory(acq_history, Y_fatigue)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Fatigue Strength\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced featurization\n",
    "The featurization above is fairly simple; we can actually bring in *much more* information about materials by leveraging our physical insight. The [Matminer](https://hackingmaterials.lbl.gov/matminer/) package is a set of tools for data-mining on chemicals data. Their library provides tools to produce _descriptors_ (features) based on chemical compositions. The following (Optional!) code demonstrates how to use Matminer to generate numerous features based on inorganic chemical labels (compositions). The following code demonstrates how to featurize chemical data using Matminer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "from matminer.featurizers.base import MultipleFeaturizer\n",
    "from matminer.featurizers import composition as cf\n",
    "from pymatgen import Composition\n",
    "\n",
    "def get_composition(c):\n",
    "    \"\"\"Attempt to parse a composition, return None if failed.\"\"\"\n",
    "    try:\n",
    "        return Composition(c)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "## Wrangle the composition strings\n",
    "df_data['composition'] = df_data['chemical_formula'].apply(get_composition)\n",
    "\n",
    "## Select features to compute\n",
    "featurizer = MultipleFeaturizer([\n",
    "    cf.Stoichiometry(),\n",
    "    cf.ElementProperty.from_preset(\"magpie\"),\n",
    "    cf.ValenceOrbital(props = ['avg']),\n",
    "    cf.IonProperty(fast = True)\n",
    "])\n",
    "\n",
    "## Run the featurizer\n",
    "X_matminer_features = np.array(featurizer.featurize_many(df_data['composition']))\n",
    "print(\"Featurization complete\")\n",
    "df_matminer_features = pd.DataFrame(\n",
    "    data = X_matminer_features,\n",
    "    columns = featurizer.feature_labels()\n",
    ")\n",
    "df_matminer_features.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Considerations\n",
    "\n",
    "__Feature Selection__: One 'hyperparameter' we have not explicitly mentioned is _the choice of features_ for the model. While additional features do provide more information, they also increase the number of parameters (internal coefficients) the model needs to learn. This can lead to overfitting in the same way we saw above. To help combat this issue, data scientists will perform [feature selection](https://en.wikipedia.org/wiki/Feature_selection) to choose the \"most informative\" features from a set, which can improve model generalizability.\n",
    "\n",
    "__Different Models__: We considered only simple polynomial models in this tutorial, but there exist many other model types. Some highlights:\n",
    "\n",
    "* [Gaussian processes](https://en.wikipedia.org/wiki/Gaussian_process)\n",
    "* [Random forests](https://en.wikipedia.org/wiki/Random_forest)\n",
    "* [Neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
