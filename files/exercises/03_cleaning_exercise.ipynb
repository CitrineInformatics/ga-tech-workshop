{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](https://github.com/CitrineInformatics/community-tools/blob/master/templates/fig/citrine_banner_2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Workshop\n",
    "\n",
    "*Authors: Enze Chen, Anirudh Kashyap, Chris Borg, Malcolm Davidson, Zachary del Rosario*\n",
    "\n",
    "This notebook walks through a data extraction, formatting, and validation exercise where we progressively spot errors in our data and fix them to improve model quality.\n",
    "\n",
    "### Learning outcomes\n",
    "By working through this notebook, you will be able to:\n",
    "* Understand proper data formatting for machine learning (ML).\n",
    "* Identify common challenges and errors when working with materials data.\n",
    "* Use standard programming tools to extract, format, and validate data for ML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "0. [Imports](#Python-package-imports)\n",
    "1. [Inspect data](#Step-1:-Load-and-inspect-the-data)\n",
    "1. [Extract data](#Step-2:-Data-extraction-with-Tabula)\n",
    "1. [Standardize headers](#Step-3:-Standardize-the-column-headers)\n",
    "1. [Impute missing values](#Step-4:-Impute-missing-values)\n",
    "1. [Generate formula](#Step-5:-Generate-a-chemical-formula)\n",
    "1. [Visualize data](#Step-6:-Visualize-the-data)\n",
    "1. [Correct data](#Step-7:-Correct-the-data-by-consulting-a-reference/expert)\n",
    "1. [Upload data](#Step-8:-Upload-the-data-to-Citrination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python package imports\n",
    "We will handle all the imports up front so errors can be caught right away. Many of the methods below are written and documented in `helper_functions.py`. Please refer to that file if you would like to learn about the implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython magic settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Third-party packages\n",
    "import pandas as pd              # library for working with tabular data\n",
    "import matplotlib.pyplot as plt  # library for visualizing data\n",
    "from workshop_utils import *   # Python file we wrote with helper functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA EXTRACTION\n",
    "\n",
    "## Step 1: Load and inspect the data\n",
    "\n",
    "[Back to ToC](#Table-of-Contents)\n",
    "\n",
    "Whenever you get data, it's always a good idea to look at the contents and structure. Using the [`pandas`](https://pandas.pydata.org/) package, we will:\n",
    "1. Read in the CSV file with the `read_csv()` function.\n",
    "2. Store it in a [`DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).\n",
    "3. Print out the first $n$ rows as a sanity check with the `head(n=)` method.\n",
    "\n",
    "Today we will *use* the pandas package, while tomorrow we'll learn it in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the data\n",
    "pd.set_option('display.max_columns', 500)  # do not truncate columns\n",
    "\n",
    "df1 = pd.read_csv(os.path.join('data', 'messy_data.csv'))\n",
    "\n",
    "# Display the first 5 rows\n",
    "df1.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Inspect the raw data\n",
    "Is there anything about the above DataFrame that looks concerning? Make at least two observations about the data above.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Below we will cover two useful methods for DataFrame objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the data type of each column\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out basic summary statistics of each column\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back and fix the issue with the column headers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data extraction with Tabula\n",
    "\n",
    "[Back to ToC](#Table-of-Contents)\n",
    "\n",
    "We will use [Tabula](https://tabula.technology/), an open-source tool for table extraction, to collect some more information about the column headers. \n",
    "\n",
    "### Q2: Extract the column name data\n",
    "In this step we will extract a *mapping* from the mysterious column names above to more human-readable names. We will need to follow precise steps to ensure the data are in the correct format for import. Follow the instructions below.\n",
    "\n",
    "**Follow these Instructions**:\n",
    "1. Please download and install Tabula if you havenâ€™t done so already. Run it.\n",
    "1. Browse for and Import the `Agrawal_table_excerpt.pdf` file.\n",
    "1. Drag a selection over the table and export the result to CSV.\n",
    "  - Make sure to **exclude** the table name `Table 1 NIMS data features`. The first row in your CSV should be `Abbreviation`, `Details`.\n",
    "1. Add a row mapping `No.` to `Sample Number`.\n",
    "  - Make sure to add this row **after** `Abbreviation, Details`.\n",
    "1. Change mapping for `Fatigue` to be `Fatigue Strength`.\n",
    "1. Save the CSV file  and move it to the `data` folder in your workshop directory.\n",
    "  - Tabula will save this as `tabula-Agrawal_table_excerpt.csv` by default; you should not need to rename the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA FORMATTING\n",
    "\n",
    "## Step 3: Standardize the column headers\n",
    "\n",
    "[Back to ToC](#Table-of-Contents)\n",
    "\n",
    "Standardizing the column headers will enable:\n",
    "* Consistent formatting across multiple datasets.\n",
    "* More descriptive names for better understanding.\n",
    "* Use of the [Template CSV Ingester](https://help.citrination.com/knowledgebase/articles/1188136-citrine-template-csv-csv) to obtain PIFs (though we will not show this here).\n",
    "\n",
    "The following Python code will take the table you just extracted and rename the column headers in your CSV file. Implementation details can be found in the [`helper_functions.py`](helper_functions.py) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a helper function to extract the mapping you extracted\n",
    "header_mapping = create_mapping_from_table(os.path.join('data', 'tabula-Agrawal_table_excerpt.csv'))\n",
    "header_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dictionary mapping can load the data and call a helper function to rename the columns.\n",
    "\n",
    "### Q3: Remap the column names\n",
    "Complete the code below to give the columns human-readable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Rename the columns\n",
    "# TODO: Complete the code below with\n",
    "# rename_columns_in_df(df = ?, mapping = ?)\n",
    "# Use the mapping we created in the previous step\n",
    "###\n",
    "\n",
    "# -- NO NEED TO CHANGE THIS CODE -----\n",
    "# Read in the current dataset into a DataFrame\n",
    "df3 = pd.read_csv(os.path.join('data', 'messy_data.csv'))\n",
    "\n",
    "# -- UNCOMMENT AND COMPLETE THIS CODE -----\n",
    "# df3 = rename_columns_in_df(df = ?, mapping = ?)\n",
    "\n",
    "\n",
    "# -- NO NEED TO CHANGE THIS CODE -----\n",
    "# Save the DataFrame to a new CSV file and show the new column headers\n",
    "# don't write row index into CSV\n",
    "df3.to_csv(os.path.join('data', 'messy_data_headers.csv'), index=False)\n",
    "df3.head(n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we just wanted to see the column names, and not the whole DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the current column headers\n",
    "df3.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Impute missing values\n",
    "\n",
    "[Back to ToC](#Table-of-Contents)\n",
    "\n",
    "The method of imputation of missing values will differ on a case-by-case basis. As explained by Agrawal et al., missing time values we'll assign a value of `0`, which is sensible.\n",
    "\n",
    "While we normally have to loop through the table and fill in blanks with `0`, you'll notice that pandas imported the blanks as `NaN`, and there is a built-in `fillna(value=)` method for DataFrames that fills `NaN` cells with the value of your choice.\n",
    "\n",
    "### Q4: Fill missing values\n",
    "Complete the code below to fill in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Fill the missing values with zeros\n",
    "# TODO: Complete the code below with \n",
    "# df.fillna(value=?)\n",
    "###\n",
    "\n",
    "# -- NO NEED TO CHANGE THIS CODE -----\n",
    "df4 = pd.read_csv(os.path.join('data', 'messy_data_headers.csv'))\n",
    "\n",
    "# -- UNCOMMENT AND COMPLETE THIS LINE -----\n",
    "# df4 = df4.fillna(value=?)\n",
    "\n",
    "\n",
    "# -- NO NEED TO CHANGE THIS CODE -----\n",
    "# Write the data with imputed values\n",
    "df4.to_csv(os.path.join('data', 'messy_data_imputed.csv'), index=False) # don't write row index into CSV\n",
    "df4.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate a chemical formula\n",
    "\n",
    "[Back to ToC](#Table-of-Contents)\n",
    "\n",
    "In order to take advantage of the [Magpie](https://www.nature.com/articles/npjcompumats201628) library's featurization, we must have a chemical formula. Therefore, we need to generate a chemical formula and append a new column to the DataFrame.\n",
    "\n",
    "The astute observer will note that the composition fractions currently *do not* add up to 100%. This is because the base element is iron, and the iron content is assumed to be the remainder of the composition. This means we need to calculate the fraction of iron in the sample and add that as a column before generating the chemical formula.\n",
    "\n",
    "### Q5: Compute the formulae\n",
    "Complete the code below to compute the chemical formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Compute the formulae\n",
    "# TODO: Complete the code below with the suggested helper functions\n",
    "###\n",
    "\n",
    "# -- NO NEED TO MODIFY THIS CODE -----\n",
    "df5 = pd.read_csv(os.path.join('data', 'messy_data_imputed.csv'))\n",
    "\n",
    "# Add a column for iron, the base metal, with: df_new = add_iron_composition(df=?)\n",
    "# Add a column for the chemical formula with: df_new = add_chemical_formula(df=?)\n",
    "# -- UNCOMMENT AND COMPLETE THIS CODE -----\n",
    "# df5 = ???\n",
    "\n",
    "\n",
    "# -- NO NEED TO MODIFY THIS CODE -----\n",
    "# Write CSV without row indices\n",
    "df5.to_csv(os.path.join('data', 'messy_data_formula.csv'), index=False)\n",
    "df5.head(n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA VALIDATION\n",
    "\n",
    "## Step 6: Visualize the data\n",
    "\n",
    "[Back to ToC](#Table-of-Contents)\n",
    "\n",
    "We will create a [histogram](https://en.wikipedia.org/wiki/Histogram) of the target values to look for suspicious rows in our data. The histogram is one of the [seven basic tools of quality](https://en.wikipedia.org/wiki/Seven_basic_tools_of_quality), and therefore is one of the fundamental tools for inspecting data.\n",
    "\n",
    "The `matplotlib.pyplot` library (which we imported above with the abbreviation `plt`) has the function `hist(data, bins=)` to plot histograms. We will learn how to use `matplotlib` in tomorrow's exercises.  For now, simply interpret the histogram below.\n",
    "\n",
    "### Q6: Interpret this histogram\n",
    "There is at least one kind of error remaining in the data; can you determine what it is, based on the following histogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the output 'PROPERTY: Fatigue Strength'\n",
    "df6 = pd.read_csv(os.path.join('data', 'messy_data_formula.csv'))\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (8, 6), 'font.size': 14})\n",
    "plt.hist(df6['PROPERTY: Fatigue Strength'], bins=30)\n",
    "\n",
    "# Some plot settings\n",
    "plt.xlabel('Fatigue Strength')\n",
    "plt.ylabel('Number of Entries')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can isolate the 'strange' cases by *filtering* our data; we demonstrate this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's fishy? Let's inspect the last few rows of the dataset\n",
    "df6[df6['PROPERTY: Fatigue Strength'] < 2].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Correct the data by consulting a reference/expert\n",
    "\n",
    "[Back to ToC](#Table-of-Contents)\n",
    "\n",
    "Now that we have identified some suspicious data, we have to go in and manually fix these entries. The correct values will have to be verified against the original data or expert intuition, or discarded entirely.\n",
    "\n",
    "Upon verification, we learn that fatigue strength values around 1.0 should really be 1000 times larger to account for the correct units. The following helper function will carry out this fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix cells where the Fatigue Strength is too small.\n",
    "cleaned_data = 'agrawal_steel_fatigue_dataset.csv'\n",
    "df7 = pd.read_csv(os.path.join('data', 'messy_data_formula.csv'))\n",
    "df7 = fix_fatigue_strength(df=df7)\n",
    "\n",
    "# don't write row index into CSV\n",
    "df7.to_csv(os.path.join('data', cleaned_data), index=False)\n",
    "df7[df7['PROPERTY: Fatigue Strength'] < 2].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did this fix the problem? Complete the following task to find out.\n",
    "\n",
    "### Q7: Double-check the data\n",
    "Copy the code from **Q6** to plot a histogram for the fatigue strength of `df7`. Interpret this new histogram -- are there any problems remaining? What observations can you make about the distribution of fatigue strength values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Plot a histogram of df7['PROPERTY: Fatigue Strength']\n",
    "# TODO: Copy and modify the code from Q6\n",
    "# TODO: List your observations about the data below.\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your observations about the fatigue data here**\n",
    "\n",
    "## Step 8: Upload the data to Citrination (OPTIONAL)\n",
    "\n",
    "[Back to ToC](#Table-of-Contents)\n",
    "\n",
    "Hooray! Now that we have finished cleaning the data, we can upload it to Citrination using the API. We could also use the [Template CSV Ingester](https://help.citrination.com/knowledgebase/articles/1188136-citrine-template-csv-csv) through the UI, but the method we show below is a bit faster. Yay, coding!\n",
    "\n",
    "To do so, we will first convert the CSV file into a PIF, which involves the use of the [`pypif`](https://github.com/CitrineInformatics/pypif) package. Though we will not discuss the code in much detail below, we encourage you to look through the accompanying Python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSV and create output file path\n",
    "df8 = pd.read_csv(os.path.join('data', cleaned_data))\n",
    "outfile = cleaned_data[:-4] + '.json'\n",
    "\n",
    "# Convert CSV to PIF and save to file\n",
    "csv_to_pifs(df=df8, fpath=os.path.join('data', outfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the PIF created, we can then use the API to upload the dataset to Citrination. Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify details for the deployment and dataset\n",
    "site = 'https://citrination.com'\n",
    "dataset_name = 'GATW Steel Fatigue Dataset ' + str(uuid4())[:6]\n",
    "dataset_desc = 'Steel fatigue dataset for the GATW 2019.'\n",
    "\n",
    "# Create a dataset and upload dataset to public Citrination\n",
    "api_key = getAPIKey()\n",
    "client = CitrinationClient(api_key=api_key, site=site)\n",
    "dataset_id = create_and_upload_data(\n",
    "    client=client,\n",
    "    fpath=os.path.join('data', outfile),\n",
    "    dataset_name=dataset_name,\n",
    "    dataset_desc=dataset_desc,\n",
    "    public_flag=False\n",
    ")\n",
    "print('Dataset successfully created and uploaded!')\n",
    "print('Find it at {}/datasets/{}'.format(site, dataset_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This concludes the Data Cleaning Workshop. Now you know some steps you can take when cleaning data for MI. When you have more time, you can go through the accompanying `workshop_utils.py` file to learn about the detailed implementations. We are happy to take any remaining questions you may have."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
